import tensorflow as tf

#  y = ğ‘Ÿğ‘’ğ‘™ğ‘¢{ğ‘Ÿğ‘’ğ‘™ğ‘¢{ğ‘Ÿğ‘’ğ‘™ğ‘¢[ğ‘‹@W1 + b1]@W2 +b2}@W3 +b3}

y = tf.random.normal([60000, 10])
print(y.shape)
x = tf.random.normal([60000, 784])

W1 = tf.Variable(tf.random.normal([784, 256]))
b1 = tf.Variable(tf.zeros([256]))
W2 = tf.Variable(tf.random.normal([256, 128]))
b2 = tf.Variable(tf.zeros([128]))
W3 = tf.Variable(tf.random.normal([128, 10]))
b3 = tf.Variable(tf.zeros([10]))

lr = 0.001
for i in range(10):
    with tf.GradientTape() as tape:
        # ç¬¬ä¸€å±‚è®¡ç®—ï¼Œ [60000, 784]@[784, 256] + [256] => [60000, 256] + [256] => [60000,256] + [60000, 256]=> [60000,256]
        h1 = x @ W1 + b1
        h1 = tf.nn.relu(h1)  # é€šè¿‡æ¿€æ´»å‡½æ•°

        # ç¬¬äºŒå±‚è®¡ç®—ï¼Œ [60000, 256]@[256, 128] + [128] => [60000, 128]
        h2 = h1 @ W2 + b2
        h2 = tf.nn.relu(h2)

        # ç¬¬äºŒå±‚è®¡ç®—ï¼Œ  [60000, 128]@[128, 10] + [10] => [60000, 10]
        h3 = h2 @ W3 + b3
        y_pred = h3
        loss = tf.square(y - y_pred)  # sum(y-out)^2
        loss = tf.reduce_mean(loss)  # è®¡ç®—ç½‘ç»œè¾“å‡ºä¸æ ‡ç­¾ä¹‹é—´çš„å‡æ–¹å·®ï¼Œ mse = mean(sum(y-out)^2)
    grads = tape.gradient(loss, [W1, b1,W2, b2,W3, b3])  # è‡ªåŠ¨æ¢¯åº¦ï¼Œéœ€è¦æ±‚æ¢¯åº¦çš„å¼ é‡æœ‰[w1, b1, w2, b2, w3, b3]

    # æ¢¯åº¦æ›´æ–°ï¼Œ assign_sub å°†å½“å‰å€¼å‡å»å‚æ•°å€¼ï¼ŒåŸåœ°æ›´æ–°
    W1.assign_sub(lr * grads[0])
    b1.assign_sub(lr * grads[1])
    W2.assign_sub(lr * grads[2])
    b2.assign_sub(lr * grads[3])
    W3.assign_sub(lr * grads[4])
    b3.assign_sub(lr * grads[5])

    print(i, 'loss:', loss.numpy())
    # print(b1)
